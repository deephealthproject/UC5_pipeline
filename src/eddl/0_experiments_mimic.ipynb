{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project DeepHealth, UC5 \"Deep Image Annotation\"\n",
    "\n",
    "Franco Alberto Cardillo, ILC-CNR (UNITO) \n",
    "<francoalberto.cardillo@ilc.cnr.it>\n",
    "\n",
    "<font color=\"yellow\">SET UP experimentations with MIMIC-CXR</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import os\n",
    "from posixpath import join\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from nltk import sent_tokenize\n",
    "import yaml\n",
    "\n",
    "from utils.vocabulary import Vocabulary\n",
    "from utils.text_collation import collate_fn_one_s, collate_fn_n_sents\n",
    "#> PATHS\n",
    "# folders with images\n",
    "# jpg_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0\"  # originals\n",
    "jpg_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized\"  # resize to 300x300\n",
    "dcm_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-dcm/physionet.org/files/mimic-cxr/2.0.0\"\n",
    "\n",
    "# metadata (report-based)\n",
    "filename_metadata = join(jpg_fld, \"mimic-cxr-2.0.0-metadata.csv\")\n",
    "# labels (image-based)\n",
    "filename_chexpert = join(jpg_fld, \"mimic-cxr-2.0.0-chexpert.csv\") #labels\n",
    "meta_fld = \"/mnt/datasets/mimic-cxr/meta\"\n",
    "\n",
    "# pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions\n",
    "def save_label_indexes(df, out_fld):\n",
    "    cols = df.columns\n",
    "    lab2idx = {}\n",
    "    idx2lab = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        lab2idx[c] = i\n",
    "        idx2lab[i] = c\n",
    "        print(f\"{i}) {c}\")\n",
    "    with open(join(out_fld, \"label2idx.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(lab2idx, fout)\n",
    "    print(f\"saved {join(out_fld, 'label2idx.yaml')}\")\n",
    "    print(f\"lab2idx with {len(lab2idx)} labels\")\n",
    "    with open(join(out_fld, \"idx2label.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(idx2lab, fout)\n",
    "    print(f\"saved {join(out_fld, 'idx2label.yaml')}\")\n",
    "#<\n",
    "\n",
    "def encode_text(sentences, vocab):\n",
    "    word_indexes = []\n",
    "    for sent in sentences.split(\".\"):\n",
    "        tokens = sent.strip().split()\n",
    "        enc_sent = []\n",
    "        for t in tokens:\n",
    "            enc_sent.append(vocab.word2idx.get(t, Vocabulary.OOV))\n",
    "        word_indexes.append(enc_sent)\n",
    "\n",
    "    return word_indexes\n",
    "#< encode_text\n",
    "\n",
    "def build_img_text_ds(ds, image_fld=None):\n",
    "    rep_ids = []\n",
    "    image_filenames = []\n",
    "    texts = []\n",
    "    for row in ds.reset_index().itertuples():\n",
    "        for fn in row.image_filename:\n",
    "            rep_ids.append(row.id)\n",
    "            if image_fld is not None:\n",
    "                image_filenames.append(join(image_fld, fn))\n",
    "            image_filenames.append(join(image_fld, fn))  # already\n",
    "            texts.append(row.text)\n",
    "    img_text_ds = pd.DataFrame()\n",
    "    img_text_ds[\"id\"] = rep_ids\n",
    "    img_text_ds[\"image_filename\"] = image_filenames\n",
    "    img_text_ds[\"text\"] = texts\n",
    "    pd.columns = [\"id\", \"image_filename\", \"text\"]\n",
    "    # display(img_text_ds.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created exp folder: /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2\n",
      "saved IMG_DS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Atelectasis</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consolidation</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edema</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fracture</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Lesion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lung Opacity</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Finding</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pleural Other</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumonia</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pneumothorax</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Support Devices</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            0                                                  1                                                  2                                                  3                                                  4\n",
       "filename                    /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...\n",
       "Atelectasis                                                               0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Cardiomegaly                                                              0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Consolidation                                                             0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Edema                                                                     0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Enlarged Cardiomediastinum                                                0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Fracture                                                                  0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Lung Lesion                                                               0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Lung Opacity                                                              0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "No Finding                                                                1.0                                                1.0                                                1.0                                                1.0                                                1.0\n",
       "Pleural Effusion                                                          0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Pleural Other                                                             0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Pneumonia                                                                 0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Pneumothorax                                                              0.0                                                0.0                                                0.0                                                0.0                                                0.0\n",
       "Support Devices                                                           0.0                                                0.0                                                0.0                                                0.0                                                0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2/label2idx.yaml\n",
      "lab2idx with 15 labels\n",
      "saved /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2/idx2label.yaml\n",
      "MERGED (reduced)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>50414267</td>\n",
       "      <td>50414267</td>\n",
       "      <td>53189527</td>\n",
       "      <td>53189527</td>\n",
       "      <td>53911762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>split</th>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_filename</th>\n",
       "      <td>files/p10/p10000032/s50414267/02aa804e-bde0afd...</td>\n",
       "      <td>files/p10/p10000032/s50414267/174413ec-4ec4c1f...</td>\n",
       "      <td>files/p10/p10000032/s53189527/2a2277a9-b0ded15...</td>\n",
       "      <td>files/p10/p10000032/s53189527/e084de3b-be89b11...</td>\n",
       "      <td>files/p10/p10000032/s53911762/68b5c4b1-227d048...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>there is no focal consolidation pleural effusi...</td>\n",
       "      <td>there is no focal consolidation pleural effusi...</td>\n",
       "      <td>the cardiac mediastinal and hilar contours are...</td>\n",
       "      <td>the cardiac mediastinal and hilar contours are...</td>\n",
       "      <td>single frontal view of the chest provided. the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0                                                  1                                                  2                                                  3                                                  4\n",
       "id                                                       50414267                                           50414267                                           53189527                                           53189527                                           53911762\n",
       "split                                                       train                                              train                                              train                                              train                                              train\n",
       "image_filename  files/p10/p10000032/s50414267/02aa804e-bde0afd...  files/p10/p10000032/s50414267/174413ec-4ec4c1f...  files/p10/p10000032/s53189527/2a2277a9-b0ded15...  files/p10/p10000032/s53189527/e084de3b-be89b11...  files/p10/p10000032/s53911762/68b5c4b1-227d048...\n",
       "text            there is no focal consolidation pleural effusi...  there is no focal consolidation pleural effusi...  the cardiac mediastinal and hilar contours are...  the cardiac mediastinal and hilar contours are...  single frontal view of the chest provided. the..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266     3269\n",
      "247     2901\n",
      "185     2104\n",
      "232     1171\n",
      "274     1164\n",
      "        ... \n",
      "1724       1\n",
      "1353       1\n",
      "1656       1\n",
      "1766       1\n",
      "1402       1\n",
      "Name: text_len, Length: 1586, dtype: int64\n",
      "EMPTY ROWS: 0\n",
      "num words: 3004, including special tokens\n",
      "word 0: <pad>\n",
      "word 1: <oov>\n",
      "word 2: <bos>\n",
      "word 3: <eos>\n",
      "word 4: the\n",
      "VOCAB SAVED in exp fld: /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2\n",
      "encoding ...\n",
      "encoded\n",
      "COLLATED\n",
      "IMG_TEXT_DS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>image_filename</th>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "      <td>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>50414267</td>\n",
       "      <td>50414267</td>\n",
       "      <td>53189527</td>\n",
       "      <td>53189527</td>\n",
       "      <td>53911762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>there is no focal consolidation pleural effusi...</td>\n",
       "      <td>there is no focal consolidation pleural effusi...</td>\n",
       "      <td>the cardiac mediastinal and hilar contours are...</td>\n",
       "      <td>the cardiac mediastinal and hilar contours are...</td>\n",
       "      <td>single frontal view of the chest provided. the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enc_text</th>\n",
       "      <td>[[10, 5, 6, 30, 81, 12, 22, 17, 41], [68, 418,...</td>\n",
       "      <td>[[10, 5, 6, 30, 81, 12, 22, 17, 41], [68, 418,...</td>\n",
       "      <td>[[4, 44, 28, 8, 45, 37, 9, 40], [18, 152, 5, 4...</td>\n",
       "      <td>[[4, 44, 28, 8, 45, 37, 9, 40], [18, 152, 5, 4...</td>\n",
       "      <td>[[400, 120, 205, 7, 4, 24, 840], [10, 5, 6, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_text</th>\n",
       "      <td>[2, 10, 5, 6, 30, 81, 12, 22, 17, 41, 3, 0]</td>\n",
       "      <td>[2, 10, 5, 6, 30, 81, 12, 22, 17, 41, 3, 0]</td>\n",
       "      <td>[2, 4, 44, 28, 8, 45, 37, 9, 40, 3, 0, 0]</td>\n",
       "      <td>[2, 4, 44, 28, 8, 45, 37, 9, 40, 3, 0, 0]</td>\n",
       "      <td>[2, 400, 120, 205, 7, 4, 24, 840, 3, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                0                                                  1                                                  2                                                  3                                                  4\n",
       "image_filename  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...  /mnt/datasets/mimic-cxr/mimic-cxr-jpg/physione...\n",
       "id                                                       50414267                                           50414267                                           53189527                                           53189527                                           53911762\n",
       "text            there is no focal consolidation pleural effusi...  there is no focal consolidation pleural effusi...  the cardiac mediastinal and hilar contours are...  the cardiac mediastinal and hilar contours are...  single frontal view of the chest provided. the...\n",
       "enc_text        [[10, 5, 6, 30, 81, 12, 22, 17, 41], [68, 418,...  [[10, 5, 6, 30, 81, 12, 22, 17, 41], [68, 418,...  [[4, 44, 28, 8, 45, 37, 9, 40], [18, 152, 5, 4...  [[4, 44, 28, 8, 45, 37, 9, 40], [18, 152, 5, 4...  [[400, 120, 205, 7, 4, 24, 840], [10, 5, 6, 30...\n",
       "target_text           [2, 10, 5, 6, 30, 81, 12, 22, 17, 41, 3, 0]        [2, 10, 5, 6, 30, 81, 12, 22, 17, 41, 3, 0]          [2, 4, 44, 28, 8, 45, 37, 9, 40, 3, 0, 0]          [2, 4, 44, 28, 8, 45, 37, 9, 40, 3, 0, 0]      [2, 400, 120, 205, 7, 4, 24, 840, 3, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_text_ds saved: /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2/img_text_dataset.pkl\n",
      "all done, exp data available in: /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2\n"
     ]
    }
   ],
   "source": [
    "# standard exp -- FIX FOR IDX 2 LABEL\n",
    "\n",
    "\n",
    "\n",
    "# create folder for the output, output files will be saved here\n",
    "EXP_FLD = \"/mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2\"\n",
    "os.makedirs(EXP_FLD, exist_ok=True)\n",
    "print(\"created exp folder:\", EXP_FLD)\n",
    "#<\n",
    "\n",
    "# ---\n",
    "# img_ds: img_filenames (relative) to 1-hot encoded labels\n",
    "img_ds = pd.read_pickle(join(meta_fld, \"img_dataset.pkl\"))\n",
    "# read dataset: all other info, including text\n",
    "ds = pd.read_pickle(join(meta_fld, \"mimic_ds.pkl\"))\n",
    "\n",
    "# print(\"IMG_DS\")\n",
    "# display(img_ds.head().T)\n",
    "# print(\"MIMIC_DS\")\n",
    "# display(ds.head().T)\n",
    "merged = img_ds.merge(ds, left_on=\"filename\", right_on=\"path\", how=\"left\")\n",
    "# print(\"MERGED\")\n",
    "# display(merged.head().T)\n",
    "\n",
    "assert merged.shape[0] == img_ds.shape[0], \"WRONG FINAL DIM\"\n",
    "\n",
    "# ! 1.\n",
    "# ! IMAGE DATASET\n",
    "# make paths absolute\n",
    "img_ds.reset_index(inplace=True, drop=False)\n",
    "img_ds[\"filename\"] = img_ds[\"filename\"].apply(lambda x: join(jpg_fld, x))\n",
    "img_ds.set_index(\"filename\")\n",
    "img_ds.to_pickle(join(EXP_FLD, \"img_dataset.pkl\"))\n",
    "print(\"saved IMG_DS\")\n",
    "display(img_ds.head().T)\n",
    "\n",
    "# ! idx2label, label2idx\n",
    "labels = img_ds.columns\n",
    "idx2label = {}\n",
    "label2idx = {}\n",
    "for i, l in enumerate(labels):\n",
    "    idx2label[i] = l\n",
    "    label2idx[l] = i\n",
    "\n",
    "with open(join(EXP_FLD, \"label2idx.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(label2idx, fout)\n",
    "print(f\"saved {join(EXP_FLD, 'label2idx.yaml')}\")\n",
    "print(f\"lab2idx with {len(label2idx)} labels\")\n",
    "with open(join(EXP_FLD, \"idx2label.yaml\"), \"w\") as fout:\n",
    "    yaml.safe_dump(idx2label, fout)\n",
    "print(f\"saved {join(EXP_FLD, 'idx2label.yaml')}\")\n",
    "\n",
    "\n",
    "# ! 2.\n",
    "# ! IMG TEXT DATASET\n",
    "keep_cols = [\"study_id\", \"path\", \"split\", \"text\"]\n",
    "merged.drop(columns=[c for c in merged.columns if c not in keep_cols], inplace=True)\n",
    "merged.columns = [\"id\", \"split\", \"image_filename\", \"text\"]\n",
    "print(\"MERGED (reduced)\")\n",
    "display(merged.head().T)\n",
    "\n",
    "merged[\"text_len\"] = merged.text.apply(lambda s: len(s))\n",
    "print(merged.text_len.value_counts())\n",
    "\n",
    "len1 = merged.text_len == 1\n",
    "print(\"EMPTY ROWS:\", nnz(len1))\n",
    "\n",
    "\n",
    "# ! encode text\n",
    "# read vocabulary\n",
    "VOCAB_FN = join(meta_fld, \"vocab_3000.pkl\")\n",
    "with open(VOCAB_FN, \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# save in exp folder\n",
    "with open(join(EXP_FLD, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "print(f\"num words: {len(vocab.word2idx)}, including special tokens\")\n",
    "for i in range(5):\n",
    "    print(f\"word {i}: {vocab.idx2word[i]}\")\n",
    "print(\"VOCAB SAVED in exp fld:\", EXP_FLD)\n",
    "\n",
    "print(\"encoding ...\")\n",
    "enc_text = merged.text.apply(encode_text, args=(vocab,))\n",
    "merged[\"enc_text\"] = enc_text\n",
    "print('encoded')\n",
    "# display(merged.head().T)\n",
    "max_tokens = 12\n",
    "merged[\"target_text\"] = enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "print(\"COLLATED\")\n",
    "# absolute paths\n",
    "merged[\"image_filename\"] = merged[\"image_filename\"].apply(lambda x: join(jpg_fld, x))\n",
    "\n",
    "img_text_ds = merged[[\"image_filename\", \"id\", \"text\", \"enc_text\", \"target_text\"]].copy(deep=True)\n",
    "# img_text_ds.set_index(\"image_filename\", inplace=True)\n",
    "img_text_ds.to_pickle(join(EXP_FLD, \"img_text_dataset.pkl\"))\n",
    "print(\"IMG_TEXT_DS\")\n",
    "display(img_text_ds.head().T)\n",
    "\n",
    "print(f\"img_text_ds saved: {join(EXP_FLD, 'img_text_dataset.pkl')}\")\n",
    "\n",
    "\n",
    "merged.loc[merged.split == \"validate\", \"split\"] = \"valid\"\n",
    "\n",
    "# now prepare ecvl dataset\n",
    "\n",
    "# # 3 training iterations\n",
    "\n",
    "# # splits\n",
    "# print( ds.split.value_counts() )\n",
    "\n",
    "# ds.rename(columns = {\"path\":\"filename\"}, inplace=True)\n",
    "# print( ds.split.value_counts() )\n",
    "\n",
    "# df_split = ds[[\"path\", \"split\"]]\n",
    "# df_split.rename(columns = {\"path\":\"filename\"}, inplace=True)\n",
    "\n",
    "# n_trainings = 3\n",
    "\n",
    "# for i in range(n_trainings):\n",
    "#     df_split.to_pickle(join(EXP_FLD, f\"split_{i}.pkl\"))\n",
    "    \n",
    "#     print(f\"saved: {join(EXP_FLD, f'split_{i}.pkl')}\")\n",
    "\n",
    "# build_ecvl_dataset(img_ds, img_text_ds):\n",
    "print(f\"all done, exp data available in: {EXP_FLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Lung Lesion</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s50414267/174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53189527/2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53189527/e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53911762/68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999733/s57132437/428e2c18-5721d8f3-35a05001-36f3d080-9053b83c.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999733/s57132437/58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9.jpg</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s55368167/58766883-376a15ce-3b323a28-6af950a0-16b793bd.jpg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s58621812/7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08.jpg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s58971208/1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e.jpg</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>362193 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                      Atelectasis  Cardiomegaly  Consolidation  Edema  Enlarged Cardiomediastinum  Fracture  Lung Lesion  Lung Opacity  No Finding  Pleural Effusion  Pleural Other  Pneumonia  Pneumothorax  Support Devices\n",
       "filename                                                                                                                                                                                                                                                                                                                                                                     \n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s50414267/174413ec-4ec4c1f7-34ea26b7-c5f994f8-79ef1962.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53189527/2a2277a9-b0ded155-c0de8eb9-c124d10e-82c5caab.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53189527/e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p10/p10000032/s53911762/68b5c4b1-227d0485-9cc38c3f-7b84ab51-4b472714.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "...                                                                                                                                                                           ...           ...            ...    ...                         ...       ...          ...           ...         ...               ...            ...        ...           ...              ...\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999733/s57132437/428e2c18-5721d8f3-35a05001-36f3d080-9053b83c.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999733/s57132437/58c403aa-35ff8bd9-73e39f54-8dc9cc5d-e0ec3fa9.jpg          0.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         1.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s55368167/58766883-376a15ce-3b323a28-6af950a0-16b793bd.jpg          1.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         0.0               0.0            0.0        0.0           0.0              0.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s58621812/7ba273af-3d290f8d-e28d0ab4-484b7a86-7fc12b08.jpg          1.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         0.0               0.0            0.0        0.0           0.0              1.0\n",
       "/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0_resized/files/p19/p19999987/s58971208/1a1fe7e3-cbac5d93-b339aeda-86bb86b5-4f31e82e.jpg          1.0           0.0            0.0    0.0                         0.0       0.0          0.0           0.0         0.0               0.0            0.0        0.0           0.0              0.0\n",
       "\n",
       "[362193 rows x 14 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N CLASSES: 14\n",
      "SPLIT train: samples 354617\n",
      "SPLIT valid: samples 2866\n",
      "SPLIT test: samples 4710\n",
      "saving ecvl dataset in yaml format...\n",
      "saved: /mnt/datasets/uc5/EXPS/mimic/std_exp_resized_2/run_0/ecvl_ds.yml\n"
     ]
    }
   ],
   "source": [
    "def build_ecvl_dataset(img_ds, img_text_ds, name=\"na\", description=\"na\"):\n",
    "    img_ds = img_ds.set_index(\"filename\")\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    display(img_ds)\n",
    "    labels = list(range(img_ds.shape[1]))\n",
    "    print(\"N CLASSES:\", len(labels))\n",
    "    \n",
    "    d = {\n",
    "        \"name\"        : name,\n",
    "        \"description\" : description,\n",
    "        \"classes\"     : labels, \n",
    "        \"images\"      : None,\n",
    "        \"split\"       : None\n",
    "    }\n",
    "    imgs = []\n",
    "    splits = [\"train\", \"valid\", \"test\"]\n",
    "    counter = {}\n",
    "    for s in splits:\n",
    "        split_ds = img_text_ds[img_text_ds.split == s]\n",
    "        n_split_samples = split_ds.shape[0]\n",
    "        counter[s] = n_split_samples\n",
    "        print(f\"SPLIT {s}: samples {n_split_samples}\")\n",
    "\n",
    "        for row in split_ds.itertuples():\n",
    "            filename = row.image_filename\n",
    "            values = img_ds.loc[filename]\n",
    "            classes = []\n",
    "            for class_idx, v in enumerate(values):\n",
    "                if v == 1:\n",
    "                    classes.append(class_idx)\n",
    "            imgs.append({\n",
    "                \"location\": filename,\n",
    "                \"label\": classes\n",
    "            })\n",
    "    d[\"images\"] = imgs\n",
    "    d[\"split\"] = dict(training=list(range(counter[\"train\"])),\n",
    "                    validation = list(range(counter[\"train\"], counter[\"train\"] + counter[\"valid\"])),\n",
    "                    test = list(range(counter[\"train\"] + counter[\"valid\"], counter[\"train\"] + counter[\"valid\"] + counter[\"test\"])))\n",
    "    return d\n",
    "\n",
    "ecvl_ds = build_ecvl_dataset(img_ds, merged)\n",
    "os.makedirs(join(EXP_FLD, \"run_0\"), exist_ok=True)\n",
    "ecvl_filename = join(EXP_FLD, \"run_0\", \"ecvl_ds.yml\")\n",
    "print(\"saving ecvl dataset in yaml format...\")\n",
    "with open( ecvl_filename, \"w\") as fout:\n",
    "            yaml.safe_dump(ecvl_ds, fout, default_flow_style=None)\n",
    "print(\"saved:\", ecvl_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">NOTICE: cells below are taken from the pytorch pipeline and cannot be used with EDDL without modifications.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! WARNING\n",
    "# ! OLD CODE: for pytorch-based pipelines\n",
    "#  EXPERIMENT - NORMAL vs REST (unbalanced)\n",
    "\n",
    "EXP_FLD = \"/opt/uc5/results/sicaai/mimic/normal_vs_rest_unbal\"\n",
    "os.makedirs(EXP_FLD, exist_ok=True)\n",
    "print(\"created exp folder:\", EXP_FLD)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import os\n",
    "from posixpath import join\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from nltk import sent_tokenize\n",
    "import yaml\n",
    "\n",
    "from utils.vocabulary import Vocabulary\n",
    "\n",
    "# folders with images\n",
    "jpg_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "dcm_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-dcm/physionet.org/files/mimic-cxr/2.0.0\"\n",
    "\n",
    "# metadata (report-based)\n",
    "filename_metadata = join(jpg_fld, \"mimic-cxr-2.0.0-metadata.csv\")\n",
    "# labels (image-based)\n",
    "filename_chexpert = join(jpg_fld, \"mimic-cxr-2.0.0-chexpert.csv\") #labels\n",
    "# output files saved here\n",
    "meta_fld = \"/mnt/datasets/mimic-cxr/meta\"\n",
    "\n",
    "\n",
    "\n",
    "# ! process labels\n",
    "img_ds = pd.read_pickle(join(meta_fld, \"img_dataset.pkl\"))\n",
    "display(img_ds.head())\n",
    "normal_col = \"No Finding\"\n",
    "\n",
    "normal_idx = img_ds[normal_col] == 1\n",
    "others_idx = img_ds[normal_col] == 0\n",
    "print(\"normal:\", nnz(normal_idx))\n",
    "print(\"others:\", nnz(others_idx))\n",
    "\n",
    "img_ds.drop(columns=[c for c in img_ds.columns if c not in [normal_col, \"filename\"]], inplace=True)\n",
    "img_ds[\"others\"] = 0\n",
    "img_ds.loc[others_idx, \"others\"] = 1\n",
    "print(\"normal:\", nnz(img_ds[normal_col] == 1))\n",
    "print(\"others:\", nnz(img_ds[\"others\"] == 1))\n",
    "\n",
    "img_ds.to_pickle(join(EXP_FLD, \"img_dataset.pkl\"))\n",
    "\n",
    "# ! label to indexes\n",
    "def save_label_indexes(df, out_fld):\n",
    "    cols = df.columns\n",
    "    lab2idx = {}\n",
    "    idx2lab = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        lab2idx[c] = i\n",
    "        idx2lab[i] = c\n",
    "        print(f\"{i}) {c}\")\n",
    "    with open(join(out_fld, \"label2idx.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(lab2idx, fout)\n",
    "    print(f\"saved {join(out_fld, 'label2idx.yaml')}\")\n",
    "    print(f\"lab2idx with {len(lab2idx)} labels\")\n",
    "    with open(join(out_fld, \"idx2label.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(idx2lab, fout)\n",
    "    print(f\"saved {join(out_fld, 'idx2label.yaml')}\")\n",
    "#<\n",
    "save_label_indexes(img_ds, EXP_FLD)\n",
    "\n",
    "# ! encode text\n",
    "# read vocabulary\n",
    "VOCAB_FN = join(meta_fld, \"vocab_3000.pkl\")\n",
    "with open(VOCAB_FN, \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# save in exp folder\n",
    "with open(join(EXP_FLD, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "print(f\"num words: {len(vocab.word2idx)}, including special tokens\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"word {i}: {vocab.idx2word[i]}\")\n",
    "\n",
    "\n",
    "# read dataset\n",
    "ds = pd.read_pickle(join(meta_fld, \"mimic_ds.pkl\"))\n",
    "display(ds.head().T)\n",
    "\n",
    "#> ENCODING\n",
    "def encode_text(sentences, vocab):\n",
    "    word_indexes = []\n",
    "    for sent in sentences.split(\".\"):\n",
    "        tokens = sent.strip().split()\n",
    "        enc_sent = []\n",
    "        for t in tokens:\n",
    "            enc_sent.append(vocab.word2idx.get(t, Vocabulary.OOV))\n",
    "        word_indexes.append(enc_sent)\n",
    "\n",
    "    return word_indexes\n",
    "\n",
    "print(\"encoding ...\")\n",
    "enc_text = ds.text.apply(encode_text, args=(vocab,))\n",
    "ds[\"enc_text\"] = enc_text\n",
    "print('encoded')\n",
    "\n",
    "display(ds.head().T)\n",
    "\n",
    "text_ds = ds[[\"study_id\", \"path\", \"text\", \"enc_text\"]]\n",
    "text_ds = text_ds.rename(columns= {\"study_id\":\"id\", \"path\":\"image_filename\"})\n",
    "display(text_ds.head().T)\n",
    "\n",
    "text_ds.to_pickle(join(EXP_FLD, \"img_text_dataset.pkl\"))\n",
    "#<\n",
    "\n",
    "\n",
    "# 3 training iterations\n",
    "\n",
    "# splits\n",
    "print( ds.split.value_counts() )\n",
    "\n",
    "ds.loc[ds.split == \"validate\", \"split\"] = \"valid\"\n",
    "ds.rename(columns = {\"path\":\"filename\"}, inplace=True)\n",
    "print( ds.split.value_counts() )\n",
    "\n",
    "df_split = ds[[\"filename\", \"split\"]]\n",
    "\n",
    "n_iter = 3\n",
    "\n",
    "for i in range(n_iter):\n",
    "    df_split.to_pickle(join(EXP_FLD, f\"split_{i}.pkl\"))\n",
    "    print(f\"SPLIT {i}\")\n",
    "    display(df_split.head())\n",
    "    df_split.filename\n",
    "    print(f\"saved: {join(EXP_FLD, f'split_{i}.pkl')}\")\n",
    "\n",
    "print(f\"all done, exp data available in: {EXP_FLD}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  EXPERIMENT - NORMAL vs REST (balanced)\n",
    "\n",
    "EXP_FLD = \"/opt/uc5/results/sicaai/mimic/normal_vs_rest_bal\"\n",
    "os.makedirs(EXP_FLD, exist_ok=True)\n",
    "print(\"created exp folder:\", EXP_FLD)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import os\n",
    "from posixpath import join\n",
    "from IPython.display import display\n",
    "from IPython.display import Image\n",
    "import pickle\n",
    "from nltk import sent_tokenize\n",
    "import yaml\n",
    "\n",
    "from utils.vocabulary import Vocabulary\n",
    "\n",
    "# folders with images\n",
    "jpg_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-jpg/physionet.org/files/mimic-cxr-jpg/2.0.0\"\n",
    "dcm_fld = \"/mnt/datasets/mimic-cxr/mimic-cxr-dcm/physionet.org/files/mimic-cxr/2.0.0\"\n",
    "\n",
    "# metadata (report-based)\n",
    "filename_metadata = join(jpg_fld, \"mimic-cxr-2.0.0-metadata.csv\")\n",
    "# labels (image-based)\n",
    "filename_chexpert = join(jpg_fld, \"mimic-cxr-2.0.0-chexpert.csv\") #labels\n",
    "# output files saved here\n",
    "meta_fld = \"/mnt/datasets/mimic-cxr/meta\"\n",
    "\n",
    "# ! process labels\n",
    "img_ds = pd.read_pickle(join(meta_fld, \"img_dataset.pkl\"))\n",
    "# display(img_ds.head())\n",
    "normal_col = \"No Finding\"\n",
    "\n",
    "normal_idx = img_ds[normal_col] == 1\n",
    "others_idx = img_ds[normal_col] == 0\n",
    "print(\"orig dataset, normal:\", nnz(normal_idx))\n",
    "print(\"full dataset, others:\", nnz(others_idx))\n",
    "print(\"full dataset, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds.drop(columns=[c for c in img_ds.columns if c not in [normal_col, \"filename\"]], inplace=True)\n",
    "img_ds[\"others\"] = 0\n",
    "img_ds.loc[others_idx, \"others\"] = 1\n",
    "\n",
    "# save in exp folder\n",
    "print(\"IMG_DS SAVED IN EXP:\")\n",
    "display(img_ds.head())\n",
    "img_ds.to_pickle(join(EXP_FLD, \"img_dataset.pkl\"))\n",
    "\n",
    "# ds needed for the column \"split\"\n",
    "ds = pd.read_pickle(join(meta_fld, \"mimic_ds.pkl\"))\n",
    "\n",
    "# join img_ds and ds\n",
    "merged = img_ds.reset_index().merge(ds[[\"path\", \"split\"]], left_on=\"filename\", right_on=\"path\", how=\"left\")\n",
    "merged.drop(columns=[\"path\"], inplace=True)\n",
    "\n",
    "n_iters = 3\n",
    "for iter in range(n_iters):\n",
    "    dfs = []\n",
    "    for g in merged.groupby([\"split\"]):\n",
    "        # print(g)\n",
    "        sub = g[1]  # dataframe\n",
    "        n_normals = int(sub[normal_col].sum())\n",
    "        print(f\"partition {g[0]}, n_normals: {n_normals}\")\n",
    "        sub = sub.groupby(normal_col).sample(n_normals)\n",
    "        print(f\"{g[0]}, n_normals: {sub[normal_col].sum()}\")\n",
    "        print(f\"{g[0]}, n_others: {sub['others'].sum()}\")\n",
    "        print()\n",
    "        dfs.append(sub)\n",
    "        #n_normals = nnz(g[normal_col] == 1)\n",
    "        #print(n_normals)\n",
    "    data = pd.concat(dfs, axis=0)\n",
    "    print(\"SPLIT:\", iter)\n",
    "    print(\"normal:\", data[normal_col].sum())\n",
    "    print(\"others:\", data[\"others\"].sum())\n",
    "    print(\"shape:\", data.shape)\n",
    "    data.loc[data.split==\"validate\", \"split\"] = \"valid\"\n",
    "    display(data)\n",
    "    save_data = data[[\"filename\", \"split\"]]\n",
    "    print(\"DATAFRAME SPLIT:\")\n",
    "    display(save_data.head())\n",
    "    save_data.to_pickle(join(EXP_FLD, f\"split_{iter}.pkl\"))\n",
    "\n",
    "# ! label to indexes\n",
    "def save_label_indexes(df, out_fld):\n",
    "    cols = df.columns\n",
    "    lab2idx = {}\n",
    "    idx2lab = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        lab2idx[c] = i\n",
    "        idx2lab[i] = c\n",
    "        print(f\"{i}) {c}\")\n",
    "    with open(join(out_fld, \"label2idx.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(lab2idx, fout)\n",
    "    print(f\"saved {join(out_fld, 'label2idx.yaml')}\")\n",
    "    print(f\"lab2idx with {len(lab2idx)} labels\")\n",
    "    with open(join(out_fld, \"idx2label.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(idx2lab, fout)\n",
    "    print(f\"saved {join(out_fld, 'idx2label.yaml')}\")\n",
    "#<\n",
    "save_label_indexes(img_ds, EXP_FLD)\n",
    "\n",
    "# ! encode text\n",
    "# read vocabulary\n",
    "VOCAB_FN = join(meta_fld, \"vocab_3000.pkl\")\n",
    "with open(VOCAB_FN, \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# save in exp folder\n",
    "with open(join(EXP_FLD, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "print(f\"num words: {len(vocab.word2idx)}, including special tokens\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(f\"word {i}: {vocab.idx2word[i]}\")\n",
    "\n",
    "\n",
    "# read dataset\n",
    "ds = pd.read_pickle(join(meta_fld, \"mimic_ds.pkl\"))\n",
    "display(ds.head().T)\n",
    "\n",
    "#> ENCODING\n",
    "def encode_text(sentences, vocab):\n",
    "    word_indexes = []\n",
    "    for sent in sentences.split(\".\"):\n",
    "        tokens = sent.strip().split()\n",
    "        enc_sent = []\n",
    "        for t in tokens:\n",
    "            enc_sent.append(vocab.word2idx.get(t, Vocabulary.OOV))\n",
    "        word_indexes.append(enc_sent)\n",
    "\n",
    "    return word_indexes\n",
    "\n",
    "print(\"encoding ...\")\n",
    "enc_text = ds.text.apply(encode_text, args=(vocab,))\n",
    "ds[\"enc_text\"] = enc_text\n",
    "print('encoded')\n",
    "\n",
    "display(ds.head().T)\n",
    "\n",
    "text_ds = ds[[\"study_id\", \"path\", \"text\", \"enc_text\"]]\n",
    "text_ds = text_ds.rename(columns= {\"study_id\":\"id\", \"path\":\"image_filename\"})\n",
    "display(text_ds.head().T)\n",
    "\n",
    "text_ds.to_pickle(join(EXP_FLD, \"img_text_dataset.pkl\"))\n",
    "#<\n",
    "\n",
    "\n",
    "# # 3 training iterations\n",
    "\n",
    "# # splits\n",
    "# print( ds.split.value_counts() )\n",
    "\n",
    "# ds.loc[ds.split == \"validate\", \"split\"] = \"valid\"\n",
    "# ds.rename(columns = {\"path\":\"filename\"}, inplace=True)\n",
    "# print( ds.split.value_counts() )\n",
    "\n",
    "# df_split = ds[[\"filename\", \"split\"]]\n",
    "\n",
    "# n_iter = 1\n",
    "\n",
    "# for i in range(n_iter):\n",
    "#     df_split.to_pickle(join(EXP_FLD, f\"split_{i}.pkl\"))\n",
    "#     print(f\"SPLIT {i}\")\n",
    "#     display(df_split.head())\n",
    "#     print(f\"saved: {join(EXP_FLD, f'split_{i}.pkl')}\")\n",
    "\n",
    "print(f\"all done, exp data available in: {EXP_FLD}\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c36fb492bbd491e41529af2aec597382d07c9f2ffbc3911cd67d485cde204aba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
