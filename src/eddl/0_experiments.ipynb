{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project DeepHealth, UC5 \"Deep Image Annotation\"\n",
    "\n",
    "Franco Alberto Cardillo, ILC-CNR (UNITO) \n",
    "\n",
    "<francoalberto.cardillo@ilc.cnr.it>\n",
    "\n",
    "<font color=\"yellow\">Use this notebook to set up experiments for the EDDL pipellines</font>:\n",
    "- select auto or MeSH terms\n",
    "- apply a threshold on minium term frequency to balance the dataset (see notebook in preproc \"SCUMBLE_THRESHOLDS\")\n",
    "- split in training-validation-test\n",
    "- prepare yaml dataset for the ECVL dataloader\n",
    "\n",
    "Notice: the same split is used for training both the convolutional and recurrent modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import count_nonzero as nnz\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from posixpath import join\n",
    "from tqdm.notebook import tqdm\n",
    "import yaml\n",
    "from utils.vocabulary import Vocabulary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.text_collation import collate_fn_one_s, collate_fn_n_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux functions\n",
    "\n",
    "def apply_threshold(image_ds, min_freq):\n",
    "    counts = image_ds.sum(axis=0)\n",
    "    iii = counts >= min_freq\n",
    "    print(f\"{nnz(iii)} labels have at least {min_freq} freq\")\n",
    "\n",
    "    keep_labels = image_ds.columns[iii]\n",
    "    drop_labels = image_ds.columns[~iii]\n",
    "    print(f\"removing {nnz(~iii)} labels\")\n",
    "    image_ds[\"misc\"] = 0\n",
    "    misc_iii = image_ds[drop_labels].sum(axis=1) > 0\n",
    "    image_ds.loc[misc_iii, \"misc\"] = 1\n",
    "    image_ds = image_ds.drop(columns=drop_labels)  # , inplace=True)\n",
    "    return image_ds\n",
    "\n",
    "def save_label_indexes(df, out_fld):\n",
    "    cols = df.columns\n",
    "    lab2idx = {}\n",
    "    idx2lab = {}\n",
    "    for i, c in enumerate(cols):\n",
    "        lab2idx[c] = i\n",
    "        idx2lab[i] = c\n",
    "        print(f\"{i}) {c}\")\n",
    "    with open(join(out_fld, \"label2idx.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(lab2idx, fout)\n",
    "    print(f\"saved {join(out_fld, 'label2idx.yaml')}\")\n",
    "    print(f\"lab2idx with {len(lab2idx)} labels\")\n",
    "    with open(join(out_fld, \"idx2label.yaml\"), \"w\") as fout:\n",
    "        yaml.safe_dump(idx2lab, fout)\n",
    "    print(f\"saved {join(out_fld, 'idx2label.yaml')}\")\n",
    "\n",
    "\n",
    "def encode_text(sentences, vocab):\n",
    "    word_indexes = []\n",
    "    for sent in sentences.split(\".\"):\n",
    "        tokens = sent.strip().split()\n",
    "        enc_sent = []\n",
    "        for t in tokens:\n",
    "            enc_sent.append(vocab.word2idx.get(t, Vocabulary.OOV))\n",
    "        word_indexes.append(enc_sent)\n",
    "\n",
    "    return word_indexes\n",
    "\n",
    "\n",
    "def build_img_text_ds(ds, image_fld):\n",
    "    rep_ids = []\n",
    "    image_filenames = []\n",
    "    texts = []\n",
    "    for row in ds.reset_index().itertuples():\n",
    "        for fn in row.image_filename:\n",
    "            rep_ids.append(row.id)\n",
    "            image_filenames.append(join(image_fld, fn))\n",
    "            texts.append(row.text)\n",
    "    img_text_ds = pd.DataFrame()\n",
    "    img_text_ds[\"id\"] = rep_ids\n",
    "    img_text_ds[\"image_filename\"] = image_filenames\n",
    "    img_text_ds[\"text\"] = texts\n",
    "    pd.columns = [\"id\", \"image_filename\", \"text\"]\n",
    "    # display(img_text_ds.head())\n",
    "\n",
    "    enc_text = img_text_ds.text.apply(encode_text, args=(vocab,))\n",
    "    img_text_ds[\"enc_text\"] = enc_text\n",
    "    return img_text_ds\n",
    "\n",
    "def to_ecvl_dataset(img_ds, filenames, image_fld, train_ids, valid_ids, test_ids, name=\"na\", description=\"na\"):\n",
    "    labels = list(range(img_ds.shape[1]))\n",
    "    print(\"N CLASSES:\", len(labels))\n",
    "\n",
    "    d = {\n",
    "        \"name\"        : name,\n",
    "        \"description\" : description,\n",
    "        \"classes\"     : labels, \n",
    "        \"images\"      : [],\n",
    "        \"split\"       : dict(training = [int(id) for id in train_ids], \n",
    "                            validation = [int(id) for id in valid_ids], \n",
    "                            test=[int(id) for id in test_ids])\n",
    "    }\n",
    "    imgs = []\n",
    "    for fn in filenames:\n",
    "        classes = []\n",
    "        values = img_ds.loc[fn]\n",
    "        for class_idx, v in enumerate(values):\n",
    "            if v == 1:\n",
    "                classes.append(class_idx)\n",
    "        # print(f\"{fn}:\", classes)\n",
    "        imgs.append({\n",
    "            \"location\": join(image_fld, fn),\n",
    "            \"label\": classes\n",
    "        })\n",
    "    d[\"images\"] = imgs\n",
    "    return d\n",
    "        \n",
    "def to_ecvl_dataset2(img_ds, image_fld, out_fld, train_ids, valid_ids, test_ids):\n",
    "    for idx, row in img_ds.iterrows():\n",
    "        classes = []\n",
    "        for class_idx, value in enumerate(row):\n",
    "            if value == 1:\n",
    "                classes.append(class_idx)\n",
    "        print(f\"{idx}:\", classes)\n",
    "\n",
    "def build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p, seed=1):\n",
    "    n_images = img_ds.shape[0]\n",
    "    img_ds2 = img_ds.reset_index()\n",
    "    filenames = np.array(img_ds2.filename.tolist())\n",
    "    normal_col = np.array(img_ds2.normal.tolist())\n",
    "    valid_p = 1 - train_p - test_p\n",
    "    print(f\"expected |train| = {int(train_p * n_images)}\")\n",
    "    print(f\"expected |valid| = {int(valid_p * n_images)}\")\n",
    "    print(f\"expected |test| = {int(test_p * n_images)}\")\n",
    "    for i in range(n_bootstraps):\n",
    "        out_fld = join(exp_fld, f\"run_{i}\")\n",
    "        os.makedirs(out_fld, exist_ok=True)\n",
    "\n",
    "        x_train, x_test = train_test_split(\n",
    "            range(n_images), stratify=normal_col, \n",
    "            train_size=train_p, test_size=test_p, \n",
    "            random_state=seed + i, shuffle=True)\n",
    "        x_valid = [idx for idx in range(n_images) if (idx not in x_train) and (idx not in x_test)]\n",
    "        print(f\"actual |train| = {len(x_train)}\")\n",
    "        print(f\"actual |valid| = {len(x_valid)}\")\n",
    "        print(f\"actual |test| = {len(x_test)}\")\n",
    "        train_ids = filenames[x_train]\n",
    "        valid_ids = filenames[x_valid]\n",
    "        test_ids = filenames[x_test]\n",
    "        # TODO: save in run not exp\n",
    "        for ids, fn in zip([train_ids, valid_ids, test_ids], [\"train_ids.txt\", \"valid_ids.txt\", \"test_ids.txt\"]):\n",
    "            with open(join(out_fld, fn), \"w\") as fout:\n",
    "                fout.write(\"\\n\".join(ids))\n",
    "            print(f\"saved {join(out_fld, fn)}: {len(ids)} images\")\n",
    "\n",
    "        ecvl_ds = to_ecvl_dataset(img_ds, filenames, img_fld, x_train, x_valid, x_test)\n",
    "        with open( join(out_fld, \"ecvl_ds.yml\"), \"w\") as fout:\n",
    "            yaml.safe_dump(ecvl_ds, fout, default_flow_style=None)\n",
    "        print(f\"saved {join(out_fld, 'ecvl_ds.yml')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "\n",
    "# mesh term, min frequecy 70, balanced on normal\n",
    "\n",
    "exp_fld = \"/mnt/datasets/uc5/EXPS/eddl/mesh_70th\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"mesh\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 70 if LABELS == \"mesh\" else 40\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "max_tokens = 12  # including bos and eos\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld)\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# auto term, min frequecy 50, balanced on normal\n",
    "\n",
    "exp_fld = \"/mnt/datasets/uc5/EXPS/eddl/auto_50th_seed23\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"auto\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 70 if LABELS == \"mesh\" else 50\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "max_tokens = 12  # including bos and eos\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld) # absolute path names\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p, seed=23)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# auto term, SEQLEN 18, min frequecy 50, balanced on normal\n",
    "max_tokens = 18  # including bos and eos\n",
    "\n",
    "exp_fld = f\"/mnt/datasets/uc5/EXPS/eddl/auto_50th_seqlen{max_tokens}\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"auto\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 70 if LABELS == \"mesh\" else 50\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld) # absolute path names\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MESH EXP\n",
    "# mesg term, min frequecy 70, balanced on normal\n",
    "\n",
    "exp_fld = \"/mnt/datasets/uc5/EXPS/eddl/mesh_70th\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"mesh\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 70 if LABELS == \"mesh\" else 50\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "max_tokens = 12  # including bos and eos\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld) # absolute path names\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MESH EXP\n",
    "# mesg term, min frequecy 130, balanced on normal\n",
    "\n",
    "exp_fld = \"/mnt/datasets/uc5/EXPS/eddl/mesh_130th\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"mesh\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 130 if LABELS == \"mesh\" else 50\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "max_tokens = 12  # including bos and eos\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld) # absolute path names\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** EXP FLD: /mnt/datasets/uc5/EXPS/eddl/final_exp\n",
      "Using terms auto, min frequency: 50\n",
      "image dataset read, shape: (7244, 523)\n",
      "45 labels have at least 50 freq\n",
      "removing 478 labels\n",
      "thresholded image dataset, shape: (7244, 46)\n",
      "IMG_DS\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>normal</th>\n",
       "      <th>aorta</th>\n",
       "      <th>arthritic changes</th>\n",
       "      <th>atelectasis</th>\n",
       "      <th>bilateral pleural effusion</th>\n",
       "      <th>cabg</th>\n",
       "      <th>calcified granuloma</th>\n",
       "      <th>calcinosis</th>\n",
       "      <th>cardiomegaly</th>\n",
       "      <th>catheterization, central venous</th>\n",
       "      <th>...</th>\n",
       "      <th>rib</th>\n",
       "      <th>rib fracture</th>\n",
       "      <th>scar</th>\n",
       "      <th>scarring</th>\n",
       "      <th>scoliosis</th>\n",
       "      <th>sternotomy</th>\n",
       "      <th>thoracic aorta</th>\n",
       "      <th>thoracic vertebrae</th>\n",
       "      <th>tortuous aorta</th>\n",
       "      <th>misc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>filename</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CXR1_1_IM-0001-3001.png</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CXR1_1_IM-0001-4001.png</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CXR10_IM-0002-1001.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CXR10_IM-0002-2001.png</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CXR100_IM-0002-1001.png</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         normal  aorta  arthritic changes  atelectasis  \\\n",
       "filename                                                                 \n",
       "CXR1_1_IM-0001-3001.png       1      0                  0            0   \n",
       "CXR1_1_IM-0001-4001.png       1      0                  0            0   \n",
       "CXR10_IM-0002-1001.png        0      0                  0            0   \n",
       "CXR10_IM-0002-2001.png        0      0                  0            0   \n",
       "CXR100_IM-0002-1001.png       1      0                  0            0   \n",
       "\n",
       "                         bilateral pleural effusion  cabg  \\\n",
       "filename                                                    \n",
       "CXR1_1_IM-0001-3001.png                           0     0   \n",
       "CXR1_1_IM-0001-4001.png                           0     0   \n",
       "CXR10_IM-0002-1001.png                            0     0   \n",
       "CXR10_IM-0002-2001.png                            0     0   \n",
       "CXR100_IM-0002-1001.png                           0     0   \n",
       "\n",
       "                         calcified granuloma  calcinosis  cardiomegaly  \\\n",
       "filename                                                                 \n",
       "CXR1_1_IM-0001-3001.png                    0           0             0   \n",
       "CXR1_1_IM-0001-4001.png                    0           0             0   \n",
       "CXR10_IM-0002-1001.png                     1           0             0   \n",
       "CXR10_IM-0002-2001.png                     1           0             0   \n",
       "CXR100_IM-0002-1001.png                    0           0             0   \n",
       "\n",
       "                         catheterization, central venous  ...  rib  \\\n",
       "filename                                                  ...        \n",
       "CXR1_1_IM-0001-3001.png                                0  ...    0   \n",
       "CXR1_1_IM-0001-4001.png                                0  ...    0   \n",
       "CXR10_IM-0002-1001.png                                 0  ...    0   \n",
       "CXR10_IM-0002-2001.png                                 0  ...    0   \n",
       "CXR100_IM-0002-1001.png                                0  ...    0   \n",
       "\n",
       "                         rib fracture  scar  scarring  scoliosis  sternotomy  \\\n",
       "filename                                                                       \n",
       "CXR1_1_IM-0001-3001.png             0     0         0          0           0   \n",
       "CXR1_1_IM-0001-4001.png             0     0         0          0           0   \n",
       "CXR10_IM-0002-1001.png              0     0         0          0           0   \n",
       "CXR10_IM-0002-2001.png              0     0         0          0           0   \n",
       "CXR100_IM-0002-1001.png             0     0         0          0           0   \n",
       "\n",
       "                         thoracic aorta  thoracic vertebrae  tortuous aorta  \\\n",
       "filename                                                                      \n",
       "CXR1_1_IM-0001-3001.png               0                   0               0   \n",
       "CXR1_1_IM-0001-4001.png               0                   0               0   \n",
       "CXR10_IM-0002-1001.png                0                   0               0   \n",
       "CXR10_IM-0002-2001.png                0                   0               0   \n",
       "CXR100_IM-0002-1001.png               0                   0               0   \n",
       "\n",
       "                         misc  \n",
       "filename                       \n",
       "CXR1_1_IM-0001-3001.png     0  \n",
       "CXR1_1_IM-0001-4001.png     0  \n",
       "CXR10_IM-0002-1001.png      0  \n",
       "CXR10_IM-0002-2001.png      0  \n",
       "CXR100_IM-0002-1001.png     0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0) normal\n",
      "1) aorta\n",
      "2) arthritic changes\n",
      "3) atelectasis\n",
      "4) bilateral pleural effusion\n",
      "5) cabg\n",
      "6) calcified granuloma\n",
      "7) calcinosis\n",
      "8) cardiomegaly\n",
      "9) catheterization, central venous\n",
      "10) congestion\n",
      "11) copd\n",
      "12) deformity\n",
      "13) degenerative change\n",
      "14) diaphragm\n",
      "15) edema\n",
      "16) effusion\n",
      "17) emphysema\n",
      "18) eventration\n",
      "19) fracture\n",
      "20) granuloma\n",
      "21) granulomatous disease\n",
      "22) hiatal hernia\n",
      "23) hyperinflation\n",
      "24) infection\n",
      "25) infiltrates\n",
      "26) nodule\n",
      "27) opacity\n",
      "28) osteophyte\n",
      "29) pleural effusion\n",
      "30) pleural thickening\n",
      "31) pneumonia\n",
      "32) pulmonary atelectasis\n",
      "33) pulmonary disease, chronic obstructive\n",
      "34) pulmonary edema\n",
      "35) pulmonary emphysema\n",
      "36) rib\n",
      "37) rib fracture\n",
      "38) scar\n",
      "39) scarring\n",
      "40) scoliosis\n",
      "41) sternotomy\n",
      "42) thoracic aorta\n",
      "43) thoracic vertebrae\n",
      "44) tortuous aorta\n",
      "45) misc\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/label2idx.yaml\n",
      "lab2idx with 46 labels\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/idx2label.yaml\n",
      "expected |train| = 5070\n",
      "expected |valid| = 1448\n",
      "expected |test| = 724\n",
      "actual |train| = 5070\n",
      "actual |valid| = 1449\n",
      "actual |test| = 725\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_0/train_ids.txt: 5070 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_0/valid_ids.txt: 1449 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_0/test_ids.txt: 725 images\n",
      "N CLASSES: 46\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_0/ecvl_ds.yml\n",
      "actual |train| = 5070\n",
      "actual |valid| = 1449\n",
      "actual |test| = 725\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_1/train_ids.txt: 5070 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_1/valid_ids.txt: 1449 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_1/test_ids.txt: 725 images\n",
      "N CLASSES: 46\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_1/ecvl_ds.yml\n",
      "actual |train| = 5070\n",
      "actual |valid| = 1449\n",
      "actual |test| = 725\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_2/train_ids.txt: 5070 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_2/valid_ids.txt: 1449 images\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_2/test_ids.txt: 725 images\n",
      "N CLASSES: 46\n",
      "saved /mnt/datasets/uc5/EXPS/eddl/final_exp/run_2/ecvl_ds.yml\n",
      "all done for exp in: /mnt/datasets/uc5/EXPS/eddl/final_exp\n"
     ]
    }
   ],
   "source": [
    "# *** \n",
    "# FINAL check before publishing the code\n",
    "# test with lstm and gru nodes\n",
    "\n",
    "# prefix in makefile for experiments with these settings: final_\n",
    "\n",
    "\n",
    "\n",
    "# auto term, min frequecy 50, balanced on normal\n",
    "\n",
    "exp_fld = \"/mnt/datasets/uc5/EXPS/eddl/final_exp\"\n",
    "os.makedirs(exp_fld, exist_ok=True)\n",
    "print(\"** EXP FLD:\", exp_fld)\n",
    "\n",
    "ds_home_fld = \"/mnt/datasets/uc5/std-dataset\"\n",
    "img_fld = join(ds_home_fld, \"image\")\n",
    "meta_fld = \"/mnt/datasets/uc5/meta/eddl/iuchest\"\n",
    "\n",
    "LABELS = \"auto\"  # \"mesh\" or \"auto\"\n",
    "min_freq = 70 if LABELS == \"mesh\" else 50\n",
    "img_ds_fn = \"img_dataset.pkl\" if LABELS == \"mesh\" else \"img_dataset_auto.pkl\"\n",
    "in_vocab_fn = \"all_vocab_1000.pkl\"\n",
    "\n",
    "# ----------------------------------------\n",
    "print(f\"Using terms {LABELS}, min frequency: {min_freq}\")\n",
    "\n",
    "img_ds = pd.read_pickle(join(meta_fld, img_ds_fn))\n",
    "print(\"image dataset read, shape:\", img_ds.shape)\n",
    "\n",
    "img_ds = apply_threshold(img_ds, min_freq)\n",
    "print(\"thresholded image dataset, shape:\", img_ds.shape)\n",
    "print(\"IMG_DS\")\n",
    "display(img_ds.head())\n",
    "\n",
    "# save img_ds\n",
    "img_ds.to_pickle(join(exp_fld, \"img_dataset.pkl\"))\n",
    "save_label_indexes(img_ds, exp_fld)\n",
    "\n",
    "# ----------------------------------------\n",
    "# process vocabulary\n",
    "with open(join(meta_fld, \"vocab_1000.pkl\"), \"rb\") as fin:\n",
    "    vocab = pickle.load(fin)\n",
    "# process vocab here\n",
    "with open(join(exp_fld, \"vocab.pkl\"), \"wb\") as fout:\n",
    "    pickle.dump(vocab, fout)\n",
    "\n",
    "# ----------------------------------------\n",
    "# encode text\n",
    "\n",
    "max_tokens = 12  # including bos and eos\n",
    "# with EDDL we need to \"collate\" text here since the dataloader manages only images\n",
    "# in PyTorch the collation is delegated to the dataloader\n",
    "\n",
    "# first, encode words with their index in the vocabulary\n",
    "ds = pd.read_pickle( join(meta_fld, \"reports_raw2.pkl\"))\n",
    "img_text_ds = build_img_text_ds(ds, img_fld) # absolute path names\n",
    "img_text_ds[\"target_text\"] = img_text_ds.enc_text.apply(lambda enc: collate_fn_one_s(enc, max_tokens=max_tokens))\n",
    "\n",
    "# display(img_text_ds.head().T)\n",
    "\n",
    "img_text_ds.to_pickle(join(exp_fld, \"img_text_dataset.pkl\"))\n",
    "\n",
    "# now prepare ecvl dataset using img_ds, img_text_ds not used for the data loader\n",
    "n_bootstraps = 3\n",
    "train_p = 0.7\n",
    "test_p = 0.1\n",
    "\n",
    "build_ecvl_dataset(img_ds, exp_fld, img_fld, n_bootstraps, train_p, test_p, seed=23)\n",
    "\n",
    "\n",
    "print(\"all done for exp in:\", exp_fld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c36fb492bbd491e41529af2aec597382d07c9f2ffbc3911cd67d485cde204aba"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 ('eddl2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
